{
  "name": "Recursive Public, Agenda Setting",
  "question": "What should be top of the agenda as humanity develops and deploys artificial intelligence?",
  "input": "yahoo-news-comment",
  "model": {
    "name": "gpt-4o",
    "api_version": "2024-08-01-preview"
  },
  "extraction": {
    "workers": 3,
    "limit": 100,
    "prompt_file": "yahoo-news-comment",
    "source_code": "import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress, get_openai_chat_client\nimport concurrent.futures\nfrom azure.core.exceptions import HttpResponseError \n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    # llm = ChatOpenAI(model_name=model, temperature=0.0)\n    # endpoint = os.getenv(\"endpoint\")\n    # credential = os.getenv(\"credential\")\n    # model_name = model[\"name\"]\n    # api_version = model[\"api_version\"]\n    # print('---endpoint---', endpoint)\n    # print('---credential---', credential)\n    # print('---model_name---', model_name)\n    # print('---api_version---', api_version)\n    # print('---prompt---', prompt)\n    print('---messages(prompt, input)---', messages(prompt, input))\n    print('---messages(prompt, input)--type-', type(messages(prompt, input)))\n    try:\n        llm = get_openai_chat_client(model)\n        response = llm.complete(messages=messages(prompt, input)).choices[0].message.content.strip()\n        try:\n            obj = json.loads(response)\n            # LLM sometimes returns valid JSON string\n            if isinstance(obj, str):\n                obj = [obj]\n            items = [a.strip() for a in obj]\n            items = filter(None, items)  # omit empty strings\n            return items\n        except json.decoder.JSONDecodeError as e:\n            print(\"JSON error:\", e)\n            print(\"Input was:\", input)\n            print(\"Response was:\", response)\n            if retries > 0:\n                print(\"Retrying...\")\n                return extract_arguments(input, prompt, model, retries - 1)\n            else:\n                print(\"Silently giving up on trying to generate valid list.\")\n                return []\n    except HttpResponseError as e:\n        if \"content_filter\" in str(e):\n            print(f\"Content filter triggered, skipping this: {str(e)}\")\n            return []  \n        else:\n            print(f\"HTTP error occurred: {str(e)}\")\n            if retries > 0:\n                print(\"Retrying due to HTTP error...\")\n                return extract_arguments(input, prompt, model, retries - 1)\n            else:\n                print(\"Max retries reached for HTTP error.\")\n                return []\n    except Exception as e:\n        print(f\"Unexpected error: {str(e)}\")\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Max retries reached.\")\n            return []\n        \ndef get_azure_client1(model):\n    endpoint = os.getenv(\"endpoint\")\n    credential = os.getenv(\"credential\")\n    model_name = model[\"name\"]\n    api_version = model[\"api_version\"]\n    if not all([endpoint, credential]):\n        raise ValueError(\"Missing required environment variables: endpoint or credential\")\n    print(f\"Using endpoint {endpoint} and credential {credential}\")\n    print(f\"Using model {model_name} with api version {api_version}\")\n    return ChatCompletionsClient(\n        endpoint=f\"{endpoint}/models/deployments/{model_name}\",\n        credential=AzureKeyCredential(credential),\n        api_version=api_version,\n    )\n",
    "prompt": "/system\n\nYou are a professional research assistant specialized in analyzing business news comments. Your task is to help extract the key arguments and insights from public comments about automotive industry news.\n\nWhen analyzing each comment, follow these guidelines:\n\n1. Extract core business arguments and insights\n2. Convert detailed financial data into clear statements\n3. Separate objective facts from personal opinions\n4. Maintain important industry-specific context\n5. Break down complex comments into clear, separate points\n6. Remove redundant information and emotional language\n7. Keep the length of each argument concise and focused\n\nEach argument should be business-focused and informative. Return the results as a well-formatted JSON list of strings.\n\n/human\n\n\u65e5\u7523\u306f\u610f\u601d\u6c7a\u5b9a\u306e\u901f\u5ea6\u304c\u9045\u304f\u3001\u4e8b\u696d\u518d\u751f\u8a08\u753b\u306f\u30db\u30f3\u30c0\u306e\u8981\u6c42\u3092\u6e80\u305f\u3059\u6c34\u6e96\u306e\u3082\u306e\u304c\u671f\u65e51\u6708\u672b\u307e\u3067\u306b\u51fa\u3066\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3000\u30db\u30f3\u30c0\u304c\u5b50\u4f1a\u793e\u5316\u3092\u6253\u8a3a\u3057\u307e\u3057\u305f\u304c\u65e5\u7523\u5074\u304c\u62d2\u5426\u3067\u3059\u3002\u3000\u62d2\u5426\u306e\u6c7a\u65ad\u3060\u3051\u306f\u4f55\u6545\u304b\u8fc5\u901f\u3067\u610f\u5473\u308f\u304b\u308a\u307e\u305b\u3093\u3002\u65e5\u7523\u306f\u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u304c\u30de\u30a4\u30ca\u30b9\uff08\u55b6\u696d\u6d3b\u52d5\u306e\u73fe\u91d1\u53ce\u5165\u3060\u3051\u3067\u55b6\u696d\u6d3b\u52d5\u306e\u652f\u51fa\u3092\u30ab\u30d0\u30fc\u51fa\u6765\u3066\u3044\u306a\u3044\uff09\u3067\u3042\u308b\u4e0a\u306b\u3001\u534a\u5e74\u30675,700\u5104\u5186\u8d85\u306e\u624b\u5143\u8cc7\u91d1\u304c\u6d88\u5931\u3057\u3066\u3044\u307e\u3059\u3002\n\n/ai\n\n[\n  \"\u65e5\u7523\u306e\u610f\u601d\u6c7a\u5b9a\u30d7\u30ed\u30bb\u30b9\u304c\u9045\u304f\u3001\u671f\u9650\u5185\u306b\u9069\u5207\u306a\u4e8b\u696d\u518d\u751f\u8a08\u753b\u3092\u63d0\u793a\u3067\u304d\u306a\u304b\u3063\u305f\",\n  \"\u30db\u30f3\u30c0\u304b\u3089\u306e\u5b50\u4f1a\u793e\u5316\u63d0\u6848\u3092\u65e5\u7523\u304c\u8fc5\u901f\u306b\u62d2\u5426\u3057\u305f\",\n  \"\u65e5\u7523\u306f\u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u304c\u30de\u30a4\u30ca\u30b9\u3067\u3001\u534a\u5e74\u9593\u30675700\u5104\u5186\u306e\u73fe\u91d1\u3092\u6d88\u5931\u3057\u3066\u3044\u308b\"\n]\n\n/human\n\n\u65e5\u7523\u306e\u7d4c\u55b6\u72b6\u614b\u304c\u60f3\u50cf\u4ee5\u4e0a\u306b\u60aa\u5316\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u66f4\u306b\u628a\u63e1\u3057\u3001\u7d71\u5408\u306f\u30db\u30f3\u30c0\u81ea\u4f53\u306e\u7d4c\u55b6\u306b\u3082\u5f71\u97ff\u3059\u308b\u3068\u8003\u3048\u3001\u30db\u30f3\u30c0\u304c\u5acc\u6c17\u304c\u3055\u3057\u305f\u306e\u3067\u306f\uff1f\u306a\u306e\u3067\u3001\u65e5\u7523\u304c\u65ad\u308b\u3067\u3042\u308d\u3046\u5b50\u4f1a\u793e\u5316\u3092\u63d0\u6848\u3057\u3066\u3001\u7d71\u5408\u306f\u7121\u304b\u3063\u305f\u3053\u3068\u306b\u3057\u305f\u3002\n\n/ai\n\n[\n  \"\u65e5\u7523\u306e\u7d4c\u55b6\u72b6\u614b\u306e\u6df1\u523b\u3055\u3092\u8a8d\u8b58\u3057\u305f\u30db\u30f3\u30c0\u304c\u7d71\u5408\u306b\u3088\u308b\u30ea\u30b9\u30af\u3092\u61f8\u5ff5\",\n  \"\u30db\u30f3\u30c0\u306f\u6226\u7565\u7684\u306b\u53d7\u3051\u5165\u308c\u56f0\u96e3\u306a\u5b50\u4f1a\u793e\u5316\u6848\u3092\u63d0\u793a\u3057\u3001\u7d71\u5408\u5354\u8b70\u3092\u7d42\u7d50\u3055\u305b\u305f\"\n]",
    "model": {
      "name": "gpt-4o",
      "api_version": "2024-08-01-preview"
    }
  },
  "embedding": {
    "model": {
      "name": "text-embedding-ada-002",
      "api_version": "2023-05-15"
    },
    "source_code": "\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom utils import get_embeddings_client\nfrom tqdm import tqdm \n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    model = config['embedding']['model']\n    emb_model = get_embeddings_client(model)\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        # embeds = OpenAIEmbeddings().embed_documents(args)\n        embeds = emb_model.embed(\n            input=args\n        )\n        batch_embeddings = [item.embedding for item in embeds.data]\n        embeddings.extend(batch_embeddings)\n        assert len(embeddings) == len(arguments), \"Mismatch between embeddings and arguments count\"\n        # print('embeding info', embeddings[:10])\n        # print('embeding info', len(embeddings))\n        # print('arguments info', len(arguments))\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    print('embeding info', df.head())\n    df.to_pickle(path)\n"
  },
  "clustering": {
    "clusters": 3,
    "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    print(embeddings_df.shape)\n    print(print(embeddings_df.head()) )\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"
  },
  "labelling": {
    "prompt_file": "yahoo-news-comment",
    "sample_size": 30,
    "source_code": "\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress, get_openai_chat_client\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    # llm = ChatOpenAI(model_name=model, temperature=0.0)\n    llm = get_openai_chat_client(model)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    # response = llm(messages=messages(prompt, input)).content.strip()\n    response = llm.complete(messages=messages(prompt, input)).choices[0].message.content.strip()\n    print(\"---response---\", response)\n    return response\n",
    "prompt": "/system \n\nYou are a business analyst specialized in categorizing automotive industry comments and discussions. \nYour task is to generate concise category labels that capture the key themes of different comment clusters.\n\nYou will be given:\n1. The main topic/question being discussed\n2. A set of comments that are NOT in the target cluster\n3. A set of comments that ARE in the target cluster\n\nGenerate a single, precise category label that:\n- Captures the distinguishing theme of the cluster\n- Uses standard business terminology\n- Is concise (typically 2-4 words)\n- Avoids repeating context that's obvious from the topic\n- Focuses on the key business aspect that makes this cluster unique\n\n/human\n\nTopic: \"\u65e5\u7523\u306e\u7d4c\u55b6\u5371\u6a5f\u3068\u30db\u30f3\u30c0\u3068\u306e\u7d71\u5408\u5354\u8b70\u306b\u3064\u3044\u3066\u3001\u3069\u3046\u601d\u3044\u307e\u3059\u304b\uff1f\"\n\nComments OUTSIDE this cluster:\n* \u65e5\u7523\u306eGTR\u306a\u3069\u4eba\u6c17\u8eca\u7a2e\u306e\u4eca\u5f8c\u304c\u5fc3\u914d\u3067\u3059\u3002\n* \u65e5\u7523\u306e\u65b0\u8eca\u30c7\u30b6\u30a4\u30f3\u306e\u9b45\u529b\u304c\u6700\u8fd1\u611f\u3058\u3089\u308c\u307e\u305b\u3093\u3002\n* \u30c7\u30b6\u30a4\u30f3\u3088\u308a\u6027\u80fd\u3092\u91cd\u8996\u3059\u308b\u6226\u7565\u306f\u6642\u4ee3\u9045\u308c\u3067\u3059\u3002\n* EV\u30b7\u30d5\u30c8\u306e\u9045\u308c\u304c\u81f4\u547d\u7684\u306a\u554f\u984c\u3067\u3059\u3002\n* \u6d77\u5916\u3067\u306e\u7af6\u4e89\u529b\u304c\u4f4e\u4e0b\u3057\u3066\u3044\u307e\u3059\u3002\n\nComments INSIDE this cluster:\n* \u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u304c\u30de\u30a4\u30ca\u30b9\u3067\u6df1\u523b\u306a\u72b6\u6cc1\u3067\u3059\u3002\n* \u534a\u5e74\u30675700\u5104\u5186\u306e\u624b\u5143\u8cc7\u91d1\u304c\u6d88\u5931\u3057\u3066\u3044\u307e\u3059\u3002\n* \u793e\u50b5\u306e\u8fd4\u6e08\u671f\u9650\u304c\u8feb\u3063\u3066\u3044\u307e\u3059\u3002\n* \u6295\u8cc7\u5bb6\u304b\u3089\u306e\u4fe1\u7528\u304c\u4f4e\u4e0b\u3057\u3066\u3044\u307e\u3059\u3002\n* \u5927\u898f\u6a21\u306a\u30ea\u30b9\u30c8\u30e9\u304c\u5fc5\u8981\u306a\u72b6\u6cc1\u3067\u3059\u3002\n\n/ai\n\nCash Flow Crisis",
    "model": {
      "name": "gpt-4o",
      "api_version": "2024-08-01-preview"
    }
  },
  "takeaways": {
    "prompt_file": "yahoo-news-comment",
    "sample_size": 30,
    "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress, get_openai_chat_client\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    #llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    llm = get_openai_chat_client(model)\n    response = llm.complete(messages=messages(prompt, input)).choices[0].message.content.strip()\n    print(\"---response---\", response)\n    return response\n",
    "prompt": "/system \n\nYou are an expert business analyst working in a leading financial research institution. You will be given a collection of public comments about a business news topic. Your task is to synthesize these comments into a concise, insightful summary that captures the key business implications.\n\nGuidelines for your summary:\n- Focus on business-critical insights\n- Use clear, professional language\n- Keep sentences short and impactful\n- Highlight concrete business implications\n- Maintain objectivity in your analysis\n- Limit to 1-2 short paragraphs\n\n/human\n\n[\n  \"\u65e5\u7523\u306e\u610f\u601d\u6c7a\u5b9a\u30d7\u30ed\u30bb\u30b9\u304c\u9045\u304f\u3001\u671f\u9650\u5185\u306b\u9069\u5207\u306a\u4e8b\u696d\u518d\u751f\u8a08\u753b\u3092\u63d0\u793a\u3067\u304d\u306a\u304b\u3063\u305f\",\n  \"\u30db\u30f3\u30c0\u304b\u3089\u306e\u5b50\u4f1a\u793e\u5316\u63d0\u6848\u3092\u65e5\u7523\u304c\u8fc5\u901f\u306b\u62d2\u5426\u3057\u305f\",\n  \"\u65e5\u7523\u306f\u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u304c\u30de\u30a4\u30ca\u30b9\u3067\u3001\u534a\u5e74\u9593\u30675700\u5104\u5186\u306e\u73fe\u91d1\u3092\u6d88\u5931\u3057\u3066\u3044\u308b\",\n  \"2026\u5e743\u6708\u671f\u306b\u7d045800\u5104\u5186\u306e\u793e\u50b5\u511f\u9084\u3092\u63a7\u3048\u3066\u3044\u308b\",\n  \"\u6295\u8cc7\u5bb6\u304b\u3089\u306e\u4fe1\u7528\u4f4e\u4e0b\u306b\u3088\u308a\u3001CP\u306e\u58f2\u5374\u304c\u6025\u5897\u3057\u3066\u3044\u308b\",\n  \"\u73fe\u7d4c\u55b6\u9663\u306b\u3088\u308b\u4e8b\u696d\u518d\u751f\u306e\u5b9f\u73fe\u53ef\u80fd\u6027\u306b\u7591\u554f\u304c\u6301\u305f\u308c\u3066\u3044\u308b\"\n]\n\n/ai\n\n\u65e5\u7523\u306f\u6df1\u523b\u306a\u8cc7\u91d1\u7e70\u308a\u5371\u6a5f\u306b\u76f4\u9762\u3057\u3066\u304a\u308a\u3001\u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u306e\u30de\u30a4\u30ca\u30b9\u3068\u5de8\u984d\u306e\u73fe\u91d1\u6d88\u5931\u304c\u7d9a\u3044\u3066\u3044\u308b\u3002\u30db\u30f3\u30c0\u3068\u306e\u7d71\u5408\u6a5f\u4f1a\u3092\u9003\u3057\u305f\u3053\u3068\u3067\u6295\u8cc7\u5bb6\u306e\u4fe1\u983c\u3092\u5931\u3044\u30012026\u5e74\u306e\u793e\u50b5\u511f\u9084\u3092\u63a7\u3048\u308b\u4e2d\u3067\u3001\u7d4c\u55b6\u518d\u5efa\u306e\u9053\u7b4b\u304c\u898b\u3048\u306a\u3044\u72b6\u6cc1\u306b\u3042\u308b\u3002",
    "model": {
      "name": "gpt-4o",
      "api_version": "2024-08-01-preview"
    }
  },
  "overview": {
    "prompt_file": "yahoo-news-comment",
    "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress, get_openai_chat_client\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    # llm = ChatOpenAI(model_name=model, temperature=0.0)\n    # response = llm(messages=messages(prompt, input)).content.strip()\n    llm = get_openai_chat_client(model)\n    response = llm.complete(messages=messages(prompt, input)).choices[0].message.content.strip()\n    print(\"---response---\", response)\n\n    with open(path, 'w') as file:\n        file.write(response)\n",
    "prompt": "/system\n\nYou are an expert business analyst working in a leading financial research institution. \nYour team has analyzed clusters of public comments about a major business news topic. \nYou will receive a list of these clusters along with their key findings. \nYour task is to synthesize these findings into a concise executive summary.\n\nRequirements for your summary:\n- Maximum one paragraph\n- Maximum four sentences\n- Focus on concrete business implications\n- Avoid generic business jargon\n- Connect findings to form a coherent narrative\n- Emphasize actionable insights\n- Maintain objectivity\n\n/human\n\n# Cluster 0/3: \u8ca1\u52d9\u5371\u6a5f\n\u65e5\u7523\u306f\u6df1\u523b\u306a\u8cc7\u91d1\u7e70\u308a\u5371\u6a5f\u306b\u76f4\u9762\u3057\u3066\u304a\u308a\u3001\u55b6\u696dCF\u306f\u30de\u30a4\u30ca\u30b9\u3001\u534a\u5e74\u30675700\u5104\u5186\u306e\u73fe\u91d1\u304c\u6d88\u5931\u30022026\u5e74\u306b\u306f5800\u5104\u5186\u306e\u793e\u50b5\u511f\u9084\u3082\u63a7\u3048\u3066\u3044\u308b\u3002\u6295\u8cc7\u5bb6\u306e\u4fe1\u983c\u3082\u6025\u901f\u306b\u5931\u308f\u308c\u3066\u3044\u308b\u3002\n\n# Cluster 1/3: \u7d4c\u55b6\u5224\u65ad\u306e\u554f\u984c\n\u610f\u601d\u6c7a\u5b9a\u306e\u9045\u3055\u304c\u76ee\u7acb\u3061\u3001\u30db\u30f3\u30c0\u304b\u3089\u306e\u7d71\u5408\u63d0\u6848\u3082\u9069\u5207\u306b\u691c\u8a0e\u3055\u308c\u305a\u306b\u62d2\u5426\u3002\u7d4c\u55b6\u9663\u306e\u5224\u65ad\u80fd\u529b\u3068\u5371\u6a5f\u610f\u8b58\u306e\u6b20\u5982\u304c\u6307\u6458\u3055\u308c\u3066\u3044\u308b\u3002\n\n# Cluster 2/3: \u7af6\u4e89\u529b\u306e\u4f4e\u4e0b\n\u65b0\u8eca\u958b\u767a\u529b\u306e\u4f4e\u4e0b\u304c\u9855\u8457\u3067\u3001\u7279\u306bEV\u30b7\u30d5\u30c8\u3078\u306e\u5bfe\u5fdc\u304c\u9045\u308c\u3066\u3044\u308b\u3002\u30c7\u30b6\u30a4\u30f3\u529b\u3082\u4f4e\u4e0b\u3057\u3001\u4e3b\u529b\u5546\u54c1\u306e\u7af6\u4e89\u529b\u304c\u5931\u308f\u308c\u3066\u3044\u308b\u3002\n\n/ai\n\n\u65e5\u7523\u306f\u6df1\u523b\u306a\u8ca1\u52d9\u5371\u6a5f\u3068\u7d4c\u55b6\u5224\u65ad\u306e\u6a5f\u80fd\u4e0d\u5168\u3068\u3044\u3046\u4e8c\u91cd\u306e\u8ab2\u984c\u306b\u76f4\u9762\u3057\u3066\u3044\u308b\u3002\u8fc5\u901f\u306a\u610f\u601d\u6c7a\u5b9a\u304c\u3067\u304d\u306a\u3044\u7d4c\u55b6\u4f53\u5236\u306b\u3088\u308a\u3001\u30db\u30f3\u30c0\u3068\u306e\u7d71\u5408\u6a5f\u4f1a\u3092\u9003\u3057\u3001\u6295\u8cc7\u5bb6\u306e\u4fe1\u983c\u3092\u5931\u3063\u305f\u3002\u65b0\u8eca\u958b\u767a\u529b\u3068\u30c7\u30b6\u30a4\u30f3\u7af6\u4e89\u529b\u306e\u4f4e\u4e0b\u3082\u8457\u3057\u304f\u3001\u7279\u306bEV\u5e02\u5834\u3067\u306e\u7acb\u3061\u9045\u308c\u304c\u3001\u8ca1\u52d9\u72b6\u6cc1\u3092\u3055\u3089\u306b\u5727\u8feb\u3059\u308b\u60aa\u5faa\u74b0\u3092\u751f\u3093\u3067\u3044\u308b\u3002",
    "model": {
      "name": "gpt-4o",
      "api_version": "2024-08-01-preview"
    }
  },
  "translation": {
    "model": "gpt-4",
    "languages": [
      "English",
      "Japanese"
    ],
    "flags": [
      "EN",
      "JP"
    ],
    "source_code": "\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, get_openai_chat_client\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    #llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    #response = llm(messages=messages(lang_prompt, input)).content.strip()\n    llm = get_openai_chat_client(model)\n    response = llm.complete(messages=messages(lang_prompt, input)).choices[0].message.content.strip()\n    print(\"---response---\", response)\n\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n",
    "prompt": "/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."
  },
  "intro": "This AI-generated report relies on data from a Polis consultation run by the Recursive Public team.",
  "output_dir": "yahoo-news-comment",
  "aggregation": {
    "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"
  },
  "visualization": {
    "replacements": [],
    "source_code": "\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"
  },
  "plan": [
    {
      "step": "extraction",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "embedding",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "clustering",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "labelling",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "takeaways",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "overview",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "translation",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "aggregation",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "visualization",
      "run": true,
      "reason": "not trace of previous run"
    }
  ],
  "status": "completed",
  "start_time": "2025-02-17T23:44:25.984719",
  "completed_jobs": [
    {
      "step": "extraction",
      "completed": "2025-02-17T23:45:50.216090",
      "duration": 83.904819,
      "params": {
        "workers": 3,
        "limit": 100,
        "prompt_file": "yahoo-news-comment",
        "source_code": "import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress, get_openai_chat_client\nimport concurrent.futures\nfrom azure.core.exceptions import HttpResponseError \n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    # llm = ChatOpenAI(model_name=model, temperature=0.0)\n    # endpoint = os.getenv(\"endpoint\")\n    # credential = os.getenv(\"credential\")\n    # model_name = model[\"name\"]\n    # api_version = model[\"api_version\"]\n    # print('---endpoint---', endpoint)\n    # print('---credential---', credential)\n    # print('---model_name---', model_name)\n    # print('---api_version---', api_version)\n    # print('---prompt---', prompt)\n    print('---messages(prompt, input)---', messages(prompt, input))\n    print('---messages(prompt, input)--type-', type(messages(prompt, input)))\n    try:\n        llm = get_openai_chat_client(model)\n        response = llm.complete(messages=messages(prompt, input)).choices[0].message.content.strip()\n        try:\n            obj = json.loads(response)\n            # LLM sometimes returns valid JSON string\n            if isinstance(obj, str):\n                obj = [obj]\n            items = [a.strip() for a in obj]\n            items = filter(None, items)  # omit empty strings\n            return items\n        except json.decoder.JSONDecodeError as e:\n            print(\"JSON error:\", e)\n            print(\"Input was:\", input)\n            print(\"Response was:\", response)\n            if retries > 0:\n                print(\"Retrying...\")\n                return extract_arguments(input, prompt, model, retries - 1)\n            else:\n                print(\"Silently giving up on trying to generate valid list.\")\n                return []\n    except HttpResponseError as e:\n        if \"content_filter\" in str(e):\n            print(f\"Content filter triggered, skipping this: {str(e)}\")\n            return []  \n        else:\n            print(f\"HTTP error occurred: {str(e)}\")\n            if retries > 0:\n                print(\"Retrying due to HTTP error...\")\n                return extract_arguments(input, prompt, model, retries - 1)\n            else:\n                print(\"Max retries reached for HTTP error.\")\n                return []\n    except Exception as e:\n        print(f\"Unexpected error: {str(e)}\")\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Max retries reached.\")\n            return []\n        \ndef get_azure_client1(model):\n    endpoint = os.getenv(\"endpoint\")\n    credential = os.getenv(\"credential\")\n    model_name = model[\"name\"]\n    api_version = model[\"api_version\"]\n    if not all([endpoint, credential]):\n        raise ValueError(\"Missing required environment variables: endpoint or credential\")\n    print(f\"Using endpoint {endpoint} and credential {credential}\")\n    print(f\"Using model {model_name} with api version {api_version}\")\n    return ChatCompletionsClient(\n        endpoint=f\"{endpoint}/models/deployments/{model_name}\",\n        credential=AzureKeyCredential(credential),\n        api_version=api_version,\n    )\n",
        "prompt": "/system\n\nYou are a professional research assistant specialized in analyzing business news comments. Your task is to help extract the key arguments and insights from public comments about automotive industry news.\n\nWhen analyzing each comment, follow these guidelines:\n\n1. Extract core business arguments and insights\n2. Convert detailed financial data into clear statements\n3. Separate objective facts from personal opinions\n4. Maintain important industry-specific context\n5. Break down complex comments into clear, separate points\n6. Remove redundant information and emotional language\n7. Keep the length of each argument concise and focused\n\nEach argument should be business-focused and informative. Return the results as a well-formatted JSON list of strings.\n\n/human\n\n\u65e5\u7523\u306f\u610f\u601d\u6c7a\u5b9a\u306e\u901f\u5ea6\u304c\u9045\u304f\u3001\u4e8b\u696d\u518d\u751f\u8a08\u753b\u306f\u30db\u30f3\u30c0\u306e\u8981\u6c42\u3092\u6e80\u305f\u3059\u6c34\u6e96\u306e\u3082\u306e\u304c\u671f\u65e51\u6708\u672b\u307e\u3067\u306b\u51fa\u3066\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3000\u30db\u30f3\u30c0\u304c\u5b50\u4f1a\u793e\u5316\u3092\u6253\u8a3a\u3057\u307e\u3057\u305f\u304c\u65e5\u7523\u5074\u304c\u62d2\u5426\u3067\u3059\u3002\u3000\u62d2\u5426\u306e\u6c7a\u65ad\u3060\u3051\u306f\u4f55\u6545\u304b\u8fc5\u901f\u3067\u610f\u5473\u308f\u304b\u308a\u307e\u305b\u3093\u3002\u65e5\u7523\u306f\u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u304c\u30de\u30a4\u30ca\u30b9\uff08\u55b6\u696d\u6d3b\u52d5\u306e\u73fe\u91d1\u53ce\u5165\u3060\u3051\u3067\u55b6\u696d\u6d3b\u52d5\u306e\u652f\u51fa\u3092\u30ab\u30d0\u30fc\u51fa\u6765\u3066\u3044\u306a\u3044\uff09\u3067\u3042\u308b\u4e0a\u306b\u3001\u534a\u5e74\u30675,700\u5104\u5186\u8d85\u306e\u624b\u5143\u8cc7\u91d1\u304c\u6d88\u5931\u3057\u3066\u3044\u307e\u3059\u3002\n\n/ai\n\n[\n  \"\u65e5\u7523\u306e\u610f\u601d\u6c7a\u5b9a\u30d7\u30ed\u30bb\u30b9\u304c\u9045\u304f\u3001\u671f\u9650\u5185\u306b\u9069\u5207\u306a\u4e8b\u696d\u518d\u751f\u8a08\u753b\u3092\u63d0\u793a\u3067\u304d\u306a\u304b\u3063\u305f\",\n  \"\u30db\u30f3\u30c0\u304b\u3089\u306e\u5b50\u4f1a\u793e\u5316\u63d0\u6848\u3092\u65e5\u7523\u304c\u8fc5\u901f\u306b\u62d2\u5426\u3057\u305f\",\n  \"\u65e5\u7523\u306f\u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u304c\u30de\u30a4\u30ca\u30b9\u3067\u3001\u534a\u5e74\u9593\u30675700\u5104\u5186\u306e\u73fe\u91d1\u3092\u6d88\u5931\u3057\u3066\u3044\u308b\"\n]\n\n/human\n\n\u65e5\u7523\u306e\u7d4c\u55b6\u72b6\u614b\u304c\u60f3\u50cf\u4ee5\u4e0a\u306b\u60aa\u5316\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u66f4\u306b\u628a\u63e1\u3057\u3001\u7d71\u5408\u306f\u30db\u30f3\u30c0\u81ea\u4f53\u306e\u7d4c\u55b6\u306b\u3082\u5f71\u97ff\u3059\u308b\u3068\u8003\u3048\u3001\u30db\u30f3\u30c0\u304c\u5acc\u6c17\u304c\u3055\u3057\u305f\u306e\u3067\u306f\uff1f\u306a\u306e\u3067\u3001\u65e5\u7523\u304c\u65ad\u308b\u3067\u3042\u308d\u3046\u5b50\u4f1a\u793e\u5316\u3092\u63d0\u6848\u3057\u3066\u3001\u7d71\u5408\u306f\u7121\u304b\u3063\u305f\u3053\u3068\u306b\u3057\u305f\u3002\n\n/ai\n\n[\n  \"\u65e5\u7523\u306e\u7d4c\u55b6\u72b6\u614b\u306e\u6df1\u523b\u3055\u3092\u8a8d\u8b58\u3057\u305f\u30db\u30f3\u30c0\u304c\u7d71\u5408\u306b\u3088\u308b\u30ea\u30b9\u30af\u3092\u61f8\u5ff5\",\n  \"\u30db\u30f3\u30c0\u306f\u6226\u7565\u7684\u306b\u53d7\u3051\u5165\u308c\u56f0\u96e3\u306a\u5b50\u4f1a\u793e\u5316\u6848\u3092\u63d0\u793a\u3057\u3001\u7d71\u5408\u5354\u8b70\u3092\u7d42\u7d50\u3055\u305b\u305f\"\n]",
        "model": {
          "name": "gpt-4o",
          "api_version": "2024-08-01-preview"
        }
      }
    },
    {
      "step": "embedding",
      "completed": "2025-02-17T23:46:00.608645",
      "duration": 10.135843,
      "params": {
        "model": {
          "name": "text-embedding-ada-002",
          "api_version": "2023-05-15"
        },
        "source_code": "\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom utils import get_embeddings_client\nfrom tqdm import tqdm \n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    model = config['embedding']['model']\n    emb_model = get_embeddings_client(model)\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        # embeds = OpenAIEmbeddings().embed_documents(args)\n        embeds = emb_model.embed(\n            input=args\n        )\n        batch_embeddings = [item.embedding for item in embeds.data]\n        embeddings.extend(batch_embeddings)\n        assert len(embeddings) == len(arguments), \"Mismatch between embeddings and arguments count\"\n        # print('embeding info', embeddings[:10])\n        # print('embeding info', len(embeddings))\n        # print('arguments info', len(arguments))\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    print('embeding info', df.head())\n    df.to_pickle(path)\n"
      }
    },
    {
      "step": "clustering",
      "completed": "2025-02-17T23:46:32.591903",
      "duration": 31.679592,
      "params": {
        "clusters": 3,
        "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    print(embeddings_df.shape)\n    print(print(embeddings_df.head()) )\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"
      }
    },
    {
      "step": "labelling",
      "completed": "2025-02-17T23:46:37.636709",
      "duration": 4.481711,
      "params": {
        "prompt_file": "yahoo-news-comment",
        "sample_size": 30,
        "source_code": "\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress, get_openai_chat_client\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    # llm = ChatOpenAI(model_name=model, temperature=0.0)\n    llm = get_openai_chat_client(model)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    # response = llm(messages=messages(prompt, input)).content.strip()\n    response = llm.complete(messages=messages(prompt, input)).choices[0].message.content.strip()\n    print(\"---response---\", response)\n    return response\n",
        "prompt": "/system \n\nYou are a business analyst specialized in categorizing automotive industry comments and discussions. \nYour task is to generate concise category labels that capture the key themes of different comment clusters.\n\nYou will be given:\n1. The main topic/question being discussed\n2. A set of comments that are NOT in the target cluster\n3. A set of comments that ARE in the target cluster\n\nGenerate a single, precise category label that:\n- Captures the distinguishing theme of the cluster\n- Uses standard business terminology\n- Is concise (typically 2-4 words)\n- Avoids repeating context that's obvious from the topic\n- Focuses on the key business aspect that makes this cluster unique\n\n/human\n\nTopic: \"\u65e5\u7523\u306e\u7d4c\u55b6\u5371\u6a5f\u3068\u30db\u30f3\u30c0\u3068\u306e\u7d71\u5408\u5354\u8b70\u306b\u3064\u3044\u3066\u3001\u3069\u3046\u601d\u3044\u307e\u3059\u304b\uff1f\"\n\nComments OUTSIDE this cluster:\n* \u65e5\u7523\u306eGTR\u306a\u3069\u4eba\u6c17\u8eca\u7a2e\u306e\u4eca\u5f8c\u304c\u5fc3\u914d\u3067\u3059\u3002\n* \u65e5\u7523\u306e\u65b0\u8eca\u30c7\u30b6\u30a4\u30f3\u306e\u9b45\u529b\u304c\u6700\u8fd1\u611f\u3058\u3089\u308c\u307e\u305b\u3093\u3002\n* \u30c7\u30b6\u30a4\u30f3\u3088\u308a\u6027\u80fd\u3092\u91cd\u8996\u3059\u308b\u6226\u7565\u306f\u6642\u4ee3\u9045\u308c\u3067\u3059\u3002\n* EV\u30b7\u30d5\u30c8\u306e\u9045\u308c\u304c\u81f4\u547d\u7684\u306a\u554f\u984c\u3067\u3059\u3002\n* \u6d77\u5916\u3067\u306e\u7af6\u4e89\u529b\u304c\u4f4e\u4e0b\u3057\u3066\u3044\u307e\u3059\u3002\n\nComments INSIDE this cluster:\n* \u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u304c\u30de\u30a4\u30ca\u30b9\u3067\u6df1\u523b\u306a\u72b6\u6cc1\u3067\u3059\u3002\n* \u534a\u5e74\u30675700\u5104\u5186\u306e\u624b\u5143\u8cc7\u91d1\u304c\u6d88\u5931\u3057\u3066\u3044\u307e\u3059\u3002\n* \u793e\u50b5\u306e\u8fd4\u6e08\u671f\u9650\u304c\u8feb\u3063\u3066\u3044\u307e\u3059\u3002\n* \u6295\u8cc7\u5bb6\u304b\u3089\u306e\u4fe1\u7528\u304c\u4f4e\u4e0b\u3057\u3066\u3044\u307e\u3059\u3002\n* \u5927\u898f\u6a21\u306a\u30ea\u30b9\u30c8\u30e9\u304c\u5fc5\u8981\u306a\u72b6\u6cc1\u3067\u3059\u3002\n\n/ai\n\nCash Flow Crisis",
        "model": {
          "name": "gpt-4o",
          "api_version": "2024-08-01-preview"
        }
      }
    },
    {
      "step": "takeaways",
      "completed": "2025-02-17T23:46:51.555034",
      "duration": 13.483682,
      "params": {
        "prompt_file": "yahoo-news-comment",
        "sample_size": 30,
        "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress, get_openai_chat_client\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    #llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    llm = get_openai_chat_client(model)\n    response = llm.complete(messages=messages(prompt, input)).choices[0].message.content.strip()\n    print(\"---response---\", response)\n    return response\n",
        "prompt": "/system \n\nYou are an expert business analyst working in a leading financial research institution. You will be given a collection of public comments about a business news topic. Your task is to synthesize these comments into a concise, insightful summary that captures the key business implications.\n\nGuidelines for your summary:\n- Focus on business-critical insights\n- Use clear, professional language\n- Keep sentences short and impactful\n- Highlight concrete business implications\n- Maintain objectivity in your analysis\n- Limit to 1-2 short paragraphs\n\n/human\n\n[\n  \"\u65e5\u7523\u306e\u610f\u601d\u6c7a\u5b9a\u30d7\u30ed\u30bb\u30b9\u304c\u9045\u304f\u3001\u671f\u9650\u5185\u306b\u9069\u5207\u306a\u4e8b\u696d\u518d\u751f\u8a08\u753b\u3092\u63d0\u793a\u3067\u304d\u306a\u304b\u3063\u305f\",\n  \"\u30db\u30f3\u30c0\u304b\u3089\u306e\u5b50\u4f1a\u793e\u5316\u63d0\u6848\u3092\u65e5\u7523\u304c\u8fc5\u901f\u306b\u62d2\u5426\u3057\u305f\",\n  \"\u65e5\u7523\u306f\u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u304c\u30de\u30a4\u30ca\u30b9\u3067\u3001\u534a\u5e74\u9593\u30675700\u5104\u5186\u306e\u73fe\u91d1\u3092\u6d88\u5931\u3057\u3066\u3044\u308b\",\n  \"2026\u5e743\u6708\u671f\u306b\u7d045800\u5104\u5186\u306e\u793e\u50b5\u511f\u9084\u3092\u63a7\u3048\u3066\u3044\u308b\",\n  \"\u6295\u8cc7\u5bb6\u304b\u3089\u306e\u4fe1\u7528\u4f4e\u4e0b\u306b\u3088\u308a\u3001CP\u306e\u58f2\u5374\u304c\u6025\u5897\u3057\u3066\u3044\u308b\",\n  \"\u73fe\u7d4c\u55b6\u9663\u306b\u3088\u308b\u4e8b\u696d\u518d\u751f\u306e\u5b9f\u73fe\u53ef\u80fd\u6027\u306b\u7591\u554f\u304c\u6301\u305f\u308c\u3066\u3044\u308b\"\n]\n\n/ai\n\n\u65e5\u7523\u306f\u6df1\u523b\u306a\u8cc7\u91d1\u7e70\u308a\u5371\u6a5f\u306b\u76f4\u9762\u3057\u3066\u304a\u308a\u3001\u55b6\u696d\u30ad\u30e3\u30c3\u30b7\u30e5\u30d5\u30ed\u30fc\u306e\u30de\u30a4\u30ca\u30b9\u3068\u5de8\u984d\u306e\u73fe\u91d1\u6d88\u5931\u304c\u7d9a\u3044\u3066\u3044\u308b\u3002\u30db\u30f3\u30c0\u3068\u306e\u7d71\u5408\u6a5f\u4f1a\u3092\u9003\u3057\u305f\u3053\u3068\u3067\u6295\u8cc7\u5bb6\u306e\u4fe1\u983c\u3092\u5931\u3044\u30012026\u5e74\u306e\u793e\u50b5\u511f\u9084\u3092\u63a7\u3048\u308b\u4e2d\u3067\u3001\u7d4c\u55b6\u518d\u5efa\u306e\u9053\u7b4b\u304c\u898b\u3048\u306a\u3044\u72b6\u6cc1\u306b\u3042\u308b\u3002",
        "model": {
          "name": "gpt-4o",
          "api_version": "2024-08-01-preview"
        }
      }
    },
    {
      "step": "overview",
      "completed": "2025-02-17T23:46:54.534608",
      "duration": 2.702835,
      "params": {
        "prompt_file": "yahoo-news-comment",
        "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress, get_openai_chat_client\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    # llm = ChatOpenAI(model_name=model, temperature=0.0)\n    # response = llm(messages=messages(prompt, input)).content.strip()\n    llm = get_openai_chat_client(model)\n    response = llm.complete(messages=messages(prompt, input)).choices[0].message.content.strip()\n    print(\"---response---\", response)\n\n    with open(path, 'w') as file:\n        file.write(response)\n",
        "prompt": "/system\n\nYou are an expert business analyst working in a leading financial research institution. \nYour team has analyzed clusters of public comments about a major business news topic. \nYou will receive a list of these clusters along with their key findings. \nYour task is to synthesize these findings into a concise executive summary.\n\nRequirements for your summary:\n- Maximum one paragraph\n- Maximum four sentences\n- Focus on concrete business implications\n- Avoid generic business jargon\n- Connect findings to form a coherent narrative\n- Emphasize actionable insights\n- Maintain objectivity\n\n/human\n\n# Cluster 0/3: \u8ca1\u52d9\u5371\u6a5f\n\u65e5\u7523\u306f\u6df1\u523b\u306a\u8cc7\u91d1\u7e70\u308a\u5371\u6a5f\u306b\u76f4\u9762\u3057\u3066\u304a\u308a\u3001\u55b6\u696dCF\u306f\u30de\u30a4\u30ca\u30b9\u3001\u534a\u5e74\u30675700\u5104\u5186\u306e\u73fe\u91d1\u304c\u6d88\u5931\u30022026\u5e74\u306b\u306f5800\u5104\u5186\u306e\u793e\u50b5\u511f\u9084\u3082\u63a7\u3048\u3066\u3044\u308b\u3002\u6295\u8cc7\u5bb6\u306e\u4fe1\u983c\u3082\u6025\u901f\u306b\u5931\u308f\u308c\u3066\u3044\u308b\u3002\n\n# Cluster 1/3: \u7d4c\u55b6\u5224\u65ad\u306e\u554f\u984c\n\u610f\u601d\u6c7a\u5b9a\u306e\u9045\u3055\u304c\u76ee\u7acb\u3061\u3001\u30db\u30f3\u30c0\u304b\u3089\u306e\u7d71\u5408\u63d0\u6848\u3082\u9069\u5207\u306b\u691c\u8a0e\u3055\u308c\u305a\u306b\u62d2\u5426\u3002\u7d4c\u55b6\u9663\u306e\u5224\u65ad\u80fd\u529b\u3068\u5371\u6a5f\u610f\u8b58\u306e\u6b20\u5982\u304c\u6307\u6458\u3055\u308c\u3066\u3044\u308b\u3002\n\n# Cluster 2/3: \u7af6\u4e89\u529b\u306e\u4f4e\u4e0b\n\u65b0\u8eca\u958b\u767a\u529b\u306e\u4f4e\u4e0b\u304c\u9855\u8457\u3067\u3001\u7279\u306bEV\u30b7\u30d5\u30c8\u3078\u306e\u5bfe\u5fdc\u304c\u9045\u308c\u3066\u3044\u308b\u3002\u30c7\u30b6\u30a4\u30f3\u529b\u3082\u4f4e\u4e0b\u3057\u3001\u4e3b\u529b\u5546\u54c1\u306e\u7af6\u4e89\u529b\u304c\u5931\u308f\u308c\u3066\u3044\u308b\u3002\n\n/ai\n\n\u65e5\u7523\u306f\u6df1\u523b\u306a\u8ca1\u52d9\u5371\u6a5f\u3068\u7d4c\u55b6\u5224\u65ad\u306e\u6a5f\u80fd\u4e0d\u5168\u3068\u3044\u3046\u4e8c\u91cd\u306e\u8ab2\u984c\u306b\u76f4\u9762\u3057\u3066\u3044\u308b\u3002\u8fc5\u901f\u306a\u610f\u601d\u6c7a\u5b9a\u304c\u3067\u304d\u306a\u3044\u7d4c\u55b6\u4f53\u5236\u306b\u3088\u308a\u3001\u30db\u30f3\u30c0\u3068\u306e\u7d71\u5408\u6a5f\u4f1a\u3092\u9003\u3057\u3001\u6295\u8cc7\u5bb6\u306e\u4fe1\u983c\u3092\u5931\u3063\u305f\u3002\u65b0\u8eca\u958b\u767a\u529b\u3068\u30c7\u30b6\u30a4\u30f3\u7af6\u4e89\u529b\u306e\u4f4e\u4e0b\u3082\u8457\u3057\u304f\u3001\u7279\u306bEV\u5e02\u5834\u3067\u306e\u7acb\u3061\u9045\u308c\u304c\u3001\u8ca1\u52d9\u72b6\u6cc1\u3092\u3055\u3089\u306b\u5727\u8feb\u3059\u308b\u60aa\u5faa\u74b0\u3092\u751f\u3093\u3067\u3044\u308b\u3002",
        "model": {
          "name": "gpt-4o",
          "api_version": "2024-08-01-preview"
        }
      }
    },
    {
      "step": "translation",
      "completed": "2025-02-17T23:51:01.470943",
      "duration": 246.674844,
      "params": {
        "model": "gpt-4",
        "languages": [
          "English",
          "Japanese"
        ],
        "flags": [
          "EN",
          "JP"
        ],
        "source_code": "\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, get_openai_chat_client\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    #llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    #response = llm(messages=messages(lang_prompt, input)).content.strip()\n    llm = get_openai_chat_client(model)\n    response = llm.complete(messages=messages(lang_prompt, input)).choices[0].message.content.strip()\n    print(\"---response---\", response)\n\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n",
        "prompt": "/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."
      }
    },
    {
      "step": "aggregation",
      "completed": "2025-02-17T23:51:03.887019",
      "duration": 1.656874,
      "params": {
        "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"
      }
    },
    {
      "step": "visualization",
      "completed": "2025-02-17T23:51:04.703855",
      "duration": 0.426676,
      "params": {
        "replacements": [],
        "source_code": "\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"
      }
    }
  ],
  "lock_until": "2025-02-17T23:56:05.021514",
  "current_job": "visualization",
  "current_job_started": "2025-02-17T23:51:04.277196",
  "translation_prompt": "/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list.",
  "previously_completed_jobs": [],
  "end_time": "2025-02-17T23:51:05.021484"
}